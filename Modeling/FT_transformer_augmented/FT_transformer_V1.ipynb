{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c0e981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lamia\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# FT-Transformer for tabular/time-series (hand-crafted features)\n",
    "# --------------------------------------------------------------\n",
    "# - Works with continuous + categorical columns\n",
    "# - Multiclass classification ready\n",
    "# - Class weights, early stopping, and simple scheduler included\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# ---------------------------\n",
    "# Config (tune these later)\n",
    "# ---------------------------\n",
    "CFG = dict(\n",
    "    d_token=192,             # token (embedding) dimension\n",
    "    n_heads=8,               # attention heads (d_token % n_heads == 0)\n",
    "    n_layers=4,              # transformer encoder layers\n",
    "    attn_dropout=0.1,\n",
    "    ff_dropout=0.2,          # feed-forward (MLP in the transformer) dropout\n",
    "    token_dropout=0.1,       # Drop tokens (feature dropout) during training\n",
    "    mlp_hidden=[256, 128],   # head MLP hidden dims\n",
    "    mlp_dropout=0.2,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=256,\n",
    "    epochs=40,\n",
    "    early_stopping_patience=8,\n",
    "    seed=42,\n",
    "    num_workers=0,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "def set_seed(seed: int):\n",
    "    import random, os\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects:\n",
    "      - df: pandas DataFrame with both train/val features already present\n",
    "      - cont_cols: list of continuous feature col names\n",
    "      - cat_cols: list of categorical feature col names (already integer-encoded 0..card-1)\n",
    "      - label_col: column name with integer class ids [0..num_classes-1]\n",
    "      - scaler: fitted StandardScaler for continuous features (optional during training; required for val/test)\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, cont_cols: List[str], cat_cols: List[str],\n",
    "                 label_col: Optional[str] = None, scaler: Optional[StandardScaler] = None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cont_cols = cont_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        self.label_col = label_col\n",
    "\n",
    "        cont = df[cont_cols].astype(float).values if cont_cols else np.zeros((len(df), 0), dtype=np.float32)\n",
    "        if scaler is not None and cont.shape[1] > 0:\n",
    "            cont = scaler.transform(cont)\n",
    "        self.cont = cont.astype(np.float32)\n",
    "\n",
    "        if cat_cols:\n",
    "            cats = []\n",
    "            for c in cat_cols:\n",
    "                # ensure integer type\n",
    "                cats.append(df[c].astype(int).values)\n",
    "            self.cats = np.stack(cats, axis=1).astype(np.int64)  # [N, n_cat]\n",
    "        else:\n",
    "            self.cats = np.zeros((len(df), 0), dtype=np.int64)\n",
    "\n",
    "        if label_col is not None:\n",
    "            self.y = df[label_col].astype(int).values\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_cont = torch.from_numpy(self.cont[idx]) if self.cont.shape[1] > 0 else torch.zeros(0)\n",
    "        x_cat = torch.from_numpy(self.cats[idx]) if self.cats.shape[1] > 0 else torch.zeros(0, dtype=torch.long)\n",
    "        if self.y is not None:\n",
    "            y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "            return x_cont, x_cat, y\n",
    "        return x_cont, x_cat\n",
    "\n",
    "# ---------------------------\n",
    "# FT-Transformer Components\n",
    "# ---------------------------\n",
    "class FeatureTokenizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Tokenizes features into a sequence of tokens:\n",
    "      [CLS] + [cat_1, ..., cat_K] + [cont_1, ..., cont_M]\n",
    "    - Each cat feature gets its own Embedding -> d_token\n",
    "    - Each cont feature gets a linear projection -> d_token (with LayerNorm)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_cont: int, cat_cardinalities: List[int], d_token: int):\n",
    "        super().__init__()\n",
    "        self.n_cont = n_cont\n",
    "        self.n_cat = len(cat_cardinalities)\n",
    "        self.d_token = d_token\n",
    "\n",
    "        # CLS token (learned)\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_token))\n",
    "        nn.init.trunc_normal_(self.cls, std=0.02)\n",
    "\n",
    "        # Cat embeddings\n",
    "        self.cat_embeds = nn.ModuleList(\n",
    "            [nn.Embedding(card, d_token) for card in cat_cardinalities]\n",
    "        )\n",
    "        for emb in self.cat_embeds:\n",
    "            nn.init.trunc_normal_(emb.weight, std=0.02)\n",
    "\n",
    "        # Continuous projection: one linear per feature (more expressive than single shared)\n",
    "        self.cont_linears = nn.ModuleList([nn.Linear(1, d_token) for _ in range(n_cont)])\n",
    "        self.cont_norm = nn.LayerNorm(d_token)\n",
    "\n",
    "    def forward(self, x_cont: torch.Tensor, x_cat: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x_cont: [B, n_cont] float\n",
    "        x_cat : [B, n_cat] long\n",
    "        returns: tokens [B, 1 + n_cat + n_cont, d_token]\n",
    "        \"\"\"\n",
    "        B = x_cont.size(0) if x_cont.ndim == 2 else x_cat.size(0)\n",
    "        tokens = []\n",
    "\n",
    "        # CLS\n",
    "        cls_tok = self.cls.expand(B, -1, -1)  # [B, 1, d]\n",
    "        tokens.append(cls_tok)\n",
    "\n",
    "        # Cat tokens\n",
    "        if self.n_cat > 0:\n",
    "            cat_tokens = []\n",
    "            for i, emb in enumerate(self.cat_embeds):\n",
    "                cat_tokens.append(emb(x_cat[:, i]))  # [B, d]\n",
    "            cat_tokens = torch.stack(cat_tokens, dim=1)  # [B, n_cat, d]\n",
    "            tokens.append(cat_tokens)\n",
    "\n",
    "        # Cont tokens\n",
    "        if self.n_cont > 0:\n",
    "            cont_tokens = []\n",
    "            for i, lin in enumerate(self.cont_linears):\n",
    "                v = lin(x_cont[:, i:i+1])  # [B, d]\n",
    "                v = self.cont_norm(v)\n",
    "                cont_tokens.append(v)\n",
    "            cont_tokens = torch.stack(cont_tokens, dim=1)  # [B, n_cont, d]\n",
    "            tokens.append(cont_tokens)\n",
    "\n",
    "        return torch.cat(tokens, dim=1)  # [B, L, d]\n",
    "\n",
    "class TokenDropout(nn.Module):\n",
    "    \"\"\"Feature (token) dropout: randomly drop non-CLS tokens during training.\"\"\"\n",
    "    def __init__(self, p: float):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.training or self.p <= 0.0:\n",
    "            return x\n",
    "        B, L, D = x.shape\n",
    "        # keep CLS (index 0)\n",
    "        mask = torch.ones(B, L, device=x.device, dtype=torch.bool)\n",
    "        drop = torch.rand(B, L-1, device=x.device) < self.p\n",
    "        mask[:, 1:] = ~drop\n",
    "        # To keep sequence length, zero-out dropped tokens (could also replace with learnable padding)\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "        return x\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_cont: int,\n",
    "        cat_cardinalities: List[int],\n",
    "        d_token: int,\n",
    "        n_heads: int,\n",
    "        n_layers: int,\n",
    "        attn_dropout: float,\n",
    "        ff_dropout: float,\n",
    "        token_dropout: float,\n",
    "        num_classes: int,\n",
    "        mlp_hidden: List[int],\n",
    "        mlp_dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = FeatureTokenizer(n_cont, cat_cardinalities, d_token)\n",
    "        self.token_dropout = TokenDropout(token_dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_token * 4,\n",
    "            dropout=ff_dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Head on CLS token\n",
    "        head_layers = []\n",
    "        in_dim = d_token\n",
    "        for h in mlp_hidden:\n",
    "            head_layers += [nn.Linear(in_dim, h), nn.ReLU(), nn.Dropout(mlp_dropout)]\n",
    "            in_dim = h\n",
    "        head_layers += [nn.Linear(in_dim, num_classes)]\n",
    "        self.head = nn.Sequential(*head_layers)\n",
    "\n",
    "        # Xavier init for head\n",
    "        for m in self.head:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x_cont: torch.Tensor, x_cat: torch.Tensor) -> torch.Tensor:\n",
    "        # Tokens: [B, L, d]\n",
    "        x = self.tokenizer(x_cont, x_cat)\n",
    "        x = self.token_dropout(x)\n",
    "        x = self.encoder(x)               # [B, L, d]\n",
    "        cls = x[:, 0, :]                  # [B, d]\n",
    "        logits = self.head(cls)           # [B, C]\n",
    "        return logits\n",
    "\n",
    "# ---------------------------\n",
    "# Training / Evaluation\n",
    "# ---------------------------\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_pred, all_true = [], []\n",
    "    for x_cont, x_cat, y in loader:\n",
    "        x_cont = x_cont.to(device)\n",
    "        x_cat = x_cat.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_cont, x_cat)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * y.size(0)\n",
    "        all_pred.append(torch.argmax(logits, dim=1).detach().cpu())\n",
    "        all_true.append(y.detach().cpu())\n",
    "\n",
    "    y_true = torch.cat(all_true).numpy()\n",
    "    y_pred = torch.cat(all_pred).numpy()\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return running_loss / len(loader.dataset), macro_f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_pred, all_true = [], []\n",
    "    for x_cont, x_cat, y in loader:\n",
    "        x_cont = x_cont.to(device)\n",
    "        x_cat = x_cat.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(x_cont, x_cat)\n",
    "        loss = criterion(logits, y)\n",
    "        running_loss += loss.item() * y.size(0)\n",
    "\n",
    "        all_pred.append(torch.argmax(logits, dim=1).cpu())\n",
    "        all_true.append(y.cpu())\n",
    "\n",
    "    y_true = torch.cat(all_true).numpy()\n",
    "    y_pred = torch.cat(all_pred).numpy()\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return running_loss / len(loader.dataset), macro_f1, (y_true, y_pred)\n",
    "\n",
    "def fit(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    criterion,\n",
    "    device,\n",
    "    epochs,\n",
    "    patience\n",
    "):\n",
    "    best_val_f1 = -1\n",
    "    best_state = None\n",
    "    patience_ctr = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tr_loss, tr_f1 = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_f1, _ = evaluate(model, val_loader, criterion, device)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.4f} f1={tr_f1:.4f} \"\n",
    "              f\"| val_loss={val_loss:.4f} f1={val_f1:.4f}\")\n",
    "\n",
    "        # Early stopping on F1\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_ctr = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    return model, best_val_f1\n",
    "\n",
    "# ---------------------------\n",
    "# Example: wiring it together\n",
    "# ---------------------------\n",
    "def build_loaders(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_val: pd.DataFrame,\n",
    "    cont_cols: List[str],\n",
    "    cat_cols: List[str],\n",
    "    label_col: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int\n",
    "):\n",
    "    # Standardize continuous on train only\n",
    "    scaler = StandardScaler()\n",
    "    if cont_cols:\n",
    "        scaler.fit(df_train[cont_cols].astype(float).values)\n",
    "\n",
    "    ds_train = TabularDataset(df_train, cont_cols, cat_cols, label_col, scaler)\n",
    "    ds_val   = TabularDataset(df_val,   cont_cols, cat_cols, label_col, scaler)\n",
    "\n",
    "    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    val_loader   = DataLoader(ds_val, batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, val_loader, scaler\n",
    "\n",
    "def infer_cardinalities(df: pd.DataFrame, cat_cols: List[str]) -> List[int]:\n",
    "    cards = []\n",
    "    for c in cat_cols:\n",
    "        cards.append(int(df[c].max()) + 1)  # assumes 0..max encoding\n",
    "    return cards\n",
    "\n",
    "def run_ft_transformer(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_val: pd.DataFrame,\n",
    "    cont_cols: List[str],\n",
    "    cat_cols: List[str],\n",
    "    label_col: str,\n",
    "    num_classes: int,\n",
    "    cfg: Dict = CFG\n",
    "):\n",
    "    set_seed(cfg[\"seed\"])\n",
    "    device = cfg[\"device\"]\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader, val_loader, scaler = build_loaders(\n",
    "        df_train, df_val, cont_cols, cat_cols, label_col,\n",
    "        batch_size=cfg[\"batch_size\"], num_workers=cfg[\"num_workers\"]\n",
    "    )\n",
    "\n",
    "    # Cardinalities\n",
    "    cat_cards = infer_cardinalities(df_train, cat_cols) if cat_cols else []\n",
    "\n",
    "    # Model\n",
    "    model = FTTransformer(\n",
    "        n_cont=len(cont_cols),\n",
    "        cat_cardinalities=cat_cards,\n",
    "        d_token=cfg[\"d_token\"],\n",
    "        n_heads=cfg[\"n_heads\"],\n",
    "        n_layers=cfg[\"n_layers\"],\n",
    "        attn_dropout=cfg[\"attn_dropout\"],\n",
    "        ff_dropout=cfg[\"ff_dropout\"],\n",
    "        token_dropout=cfg[\"token_dropout\"],\n",
    "        num_classes=num_classes,\n",
    "        mlp_hidden=cfg[\"mlp_hidden\"],\n",
    "        mlp_dropout=cfg[\"mlp_dropout\"],\n",
    "    ).to(device)\n",
    "\n",
    "    # Class weights (optional but useful for imbalance)\n",
    "    y_train = df_train[label_col].astype(int).values\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "    # Map to full range [0..num_classes-1]\n",
    "    cw_vec = np.ones(num_classes, dtype=np.float32)\n",
    "    for i, c in enumerate(classes):\n",
    "        cw_vec[c] = class_weights[i]\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(cw_vec, dtype=torch.float32, device=device))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg[\"epochs\"])\n",
    "\n",
    "    model, best_val_f1 = fit(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=cfg[\"epochs\"],\n",
    "        patience=cfg[\"early_stopping_patience\"]\n",
    "    )\n",
    "\n",
    "    # Final val report\n",
    "    _, _, (y_true, y_pred) = evaluate(model, val_loader, criterion, device)\n",
    "    print(\"\\nValidation classification report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    return model, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c20e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lamia\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\lamia\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=1.0023 f1=0.5884 | val_loss=1.9947 f1=0.3712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lamia\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\lamia\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 | train_loss=0.5938 f1=0.7560 | val_loss=2.0737 f1=0.4111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lamia\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\lamia\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 | train_loss=0.4806 f1=0.8061 | val_loss=2.4612 f1=0.4086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lamia\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load your dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(Path(\"C:/Users/lamia/Downloads/augmented_dataset1_trainval.csv\"))\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Define label & drop columns\n",
    "# -----------------------------\n",
    "label_cols = ['oestrus', 'calving', 'lameness', 'mastitis', 'other_disease', 'OK']\n",
    "drop_cols  = ['cow', 'duration_hours']   # will be removed from feature set\n",
    "\n",
    "# Build a single multiclass target from the one-hot columns\n",
    "# (argmax over the one-hot columns; ensure they are numeric 0/1)\n",
    "labels_matrix = df[label_cols].astype(float).values\n",
    "y_int = labels_matrix.argmax(axis=1)\n",
    "df['target'] = y_int\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Train/Val split (grouped by cow if available)\n",
    "# -----------------------------\n",
    "if 'cow' in df.columns:\n",
    "    # Grouped split to prevent leakage across cows\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "    groups = df['cow']\n",
    "    train_idx, val_idx = next(gss.split(df, y_int, groups))\n",
    "else:\n",
    "    # Stratified split on labels\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "    train_idx, val_idx = next(sss.split(df, y_int))\n",
    "\n",
    "df_train = df.iloc[train_idx].copy()\n",
    "df_val   = df.iloc[val_idx].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Build feature lists\n",
    "# -----------------------------\n",
    "# Remove labels & drop_cols from the feature space\n",
    "excluded = set(label_cols + drop_cols + ['target'])\n",
    "candidate_cols = [c for c in df.columns if c not in excluded]\n",
    "\n",
    "# Continuous columns = numeric dtypes\n",
    "cont_cols = [c for c in candidate_cols if np.issubdtype(df[c].dtype, np.number)]\n",
    "\n",
    "# Categorical columns (integer-encoded 0..card-1).\n",
    "# If you have encoded IDs like 'cow_id_enc', add them here.\n",
    "cat_cols = []  # e.g., ['cow_id_enc', 'month_enc'] if present & integer-encoded\n",
    "\n",
    "# Sanity check\n",
    "assert len(cont_cols) + len(cat_cols) > 0, \"No features found. Check your drop/feature lists.\"\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Configure FT-Transformer\n",
    "# -----------------------------\n",
    "CFG.update({\n",
    "    \"d_token\": 192,\n",
    "    \"n_heads\": 8,                 # must divide d_token\n",
    "    \"n_layers\": 4,\n",
    "    \"attn_dropout\": 0.10,\n",
    "    \"ff_dropout\": 0.20,\n",
    "    \"token_dropout\": 0.10,\n",
    "    \"mlp_hidden\": [512, 256, 128],\n",
    "    \"mlp_dropout\": 0.30,\n",
    "\n",
    "    \"batch_size\": 128,\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"epochs\": 200,\n",
    "    \"early_stopping_patience\": 15,\n",
    "})\n",
    "\n",
    "num_classes = len(label_cols)\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Train\n",
    "# -----------------------------\n",
    "model, scaler = run_ft_transformer(\n",
    "    df_train=df_train,\n",
    "    df_val=df_val,\n",
    "    cont_cols=cont_cols,\n",
    "    cat_cols=cat_cols,\n",
    "    label_col='target',\n",
    "    num_classes=num_classes,\n",
    "    cfg=CFG\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 7) OPTIONAL: Temperature scaling for calibrated probs\n",
    "#    (Good when you later want reliable confidence estimates)\n",
    "# -----------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TemperatureScaler(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.T = nn.Parameter(torch.ones(1) * 1.0)\n",
    "\n",
    "    def forward(self, x_cont, x_cat):\n",
    "        logits = self.model(x_cont, x_cat)\n",
    "        return logits / self.T.clamp(min=1e-6)\n",
    "\n",
    "    def fit(self, loader, device):\n",
    "        self.to(device)\n",
    "        self.model.eval()\n",
    "        nll = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.LBFGS([self.T], lr=0.01, max_iter=50)\n",
    "\n",
    "        logits_list, labels_list = [], []\n",
    "        with torch.no_grad():\n",
    "            for xc, xa, y in loader:\n",
    "                xc, xa, y = xc.to(device), xa.to(device), y.to(device)\n",
    "                logits_list.append(self.model(xc, xa))\n",
    "                labels_list.append(y)\n",
    "        logits = torch.cat(logits_list)\n",
    "        labels = torch.cat(labels_list)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = nll(logits / self.T.clamp(min=1e-6), labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        print(f\"Fitted temperature: {self.T.item():.4f}\")\n",
    "\n",
    "# Build a val loader with the same scaler to fit T\n",
    "val_ds = TabularDataset(df_val, cont_cols, cat_cols, label_col='target', scaler=scaler)\n",
    "val_loader = DataLoader(val_ds, batch_size=CFG[\"batch_size\"], shuffle=False)\n",
    "\n",
    "temp_model = TemperatureScaler(model)\n",
    "temp_model.fit(val_loader, device=CFG[\"device\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Example: get calibrated predictions on validation\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "temp_model.eval()\n",
    "all_probs, all_preds, all_true = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xc, xa, y in val_loader:\n",
    "        xc, xa = xc.to(CFG[\"device\"]), xa.to(CFG[\"device\"])\n",
    "        logits = temp_model(xc, xa)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_preds.append(probs.argmax(axis=1))\n",
    "        all_true.append(y.numpy())\n",
    "\n",
    "all_probs = np.vstack(all_probs)\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_true  = np.concatenate(all_true)\n",
    "\n",
    "print(\"\\nValidation (calibrated) report:\")\n",
    "print(classification_report(all_true, all_preds, target_names=label_cols, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c0177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
