{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ======================= CELL 1.5: GRID SEARCH =======================\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_single_config(df_train, df_val, cont_cols, cat_cols, label_col, num_classes, base_cfg, config):\n",
    "    \"\"\"Run a single configuration and return results.\"\"\"\n",
    "    # Merge base config with current grid config\n",
    "    current_cfg = {**base_cfg, **config}\n",
    "    \n",
    "    try:\n",
    "        model, scaler, history = run_ft_transformer(\n",
    "            df_train=df_train, df_val=df_val,\n",
    "            cont_cols=cont_cols, cat_cols=cat_cols,\n",
    "            label_col=label_col, num_classes=num_classes, \n",
    "            cfg=current_cfg\n",
    "        )\n",
    "        \n",
    "        # Extract best validation F1 and corresponding train F1\n",
    "        best_epoch = history[\"best_epoch\"] - 1  # Convert to 0-indexed\n",
    "        best_val_f1 = history[\"val_f1\"][best_epoch]\n",
    "        best_train_f1 = history[\"train_f1\"][best_epoch]\n",
    "        best_val_loss = history[\"val_loss\"][best_epoch]\n",
    "        \n",
    "        # Calculate overfitting gap\n",
    "        overfitting_gap = best_train_f1 - best_val_f1\n",
    "        \n",
    "        return {\n",
    "            'config': config,\n",
    "            'best_val_f1': best_val_f1,\n",
    "            'best_train_f1': best_train_f1,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'overfitting_gap': overfitting_gap,\n",
    "            'best_epoch': history[\"best_epoch\"],\n",
    "            'history': history,\n",
    "            'status': 'success'\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Config {config} failed with error: {str(e)}\")\n",
    "        return {\n",
    "            'config': config,\n",
    "            'best_val_f1': -1.0,\n",
    "            'best_train_f1': -1.0,\n",
    "            'best_val_loss': float('inf'),\n",
    "            'overfitting_gap': 0.0,\n",
    "            'best_epoch': 0,\n",
    "            'history': None,\n",
    "            'status': 'failed',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def grid_search_ft_transformer(df_train, df_val, cont_cols, cat_cols, label_col, num_classes, \n",
    "                              base_cfg, param_grid, max_runs=None, sort_by='val_f1'):\n",
    "    \"\"\"\n",
    "    Perform grid search for FT-Transformer hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        param_grid: Dictionary with parameter names and lists of values to try\n",
    "        max_runs: Maximum number of configurations to run (None for all)\n",
    "        sort_by: Metric to sort results by ('val_f1', 'overfitting_gap', 'val_loss')\n",
    "    \"\"\"\n",
    "    print(\"Starting Grid Search...\")\n",
    "    print(f\"Parameter grid: {param_grid}\")\n",
    "    print(f\"Total possible combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    \n",
    "    # Generate all parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    all_combinations = list(itertools.product(*param_values))\n",
    "    \n",
    "    if max_runs and len(all_combinations) > max_runs:\n",
    "        print(f\"Sampling {max_runs} random combinations from {len(all_combinations)} total...\")\n",
    "        rng = np.random.RandomState(base_cfg.get(\"seed\", 42))\n",
    "        indices = rng.choice(len(all_combinations), size=max_runs, replace=False)\n",
    "        combinations = [all_combinations[i] for i in indices]\n",
    "    else:\n",
    "        combinations = all_combinations\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, comb in enumerate(combinations):\n",
    "        config_dict = dict(zip(param_names, comb))\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Run {i+1}/{len(combinations)}: {config_dict}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = run_single_config(\n",
    "            df_train, df_val, cont_cols, cat_cols, label_col, num_classes,\n",
    "            base_cfg, config_dict\n",
    "        )\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            print(f\"âœ“ Val F1: {result['best_val_f1']:.4f} | \"\n",
    "                  f\"Train F1: {result['best_train_f1']:.4f} | \"\n",
    "                  f\"Overfitting: {result['overfitting_gap']:.4f} | \"\n",
    "                  f\"Best Epoch: {result['best_epoch']}\")\n",
    "        else:\n",
    "            print(f\"âœ— Failed: {result.get('error', 'Unknown error')}\")\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    # Filter successful runs and sort\n",
    "    successful_results = [r for r in results if r['status'] == 'success']\n",
    "    \n",
    "    if not successful_results:\n",
    "        print(\"No successful runs! Check your parameter ranges.\")\n",
    "        return results\n",
    "    \n",
    "    # Sort by specified metric\n",
    "    reverse = sort_by != 'val_loss'  # Higher is better for F1, lower for loss\n",
    "    successful_results.sort(key=lambda x: x[sort_by], reverse=reverse)\n",
    "    \n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"GRID SEARCH COMPLETE - Top 5 Configurations (sorted by {sort_by}):\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    for i, result in enumerate(successful_results[:5]):\n",
    "        print(f\"\\nRank {i+1}:\")\n",
    "        print(f\"  Config: {result['config']}\")\n",
    "        print(f\"  Val F1: {result['best_val_f1']:.4f}\")\n",
    "        print(f\"  Train F1: {result['best_train_f1']:.4f}\")\n",
    "        print(f\"  Overfitting gap: {result['overfitting_gap']:.4f}\")\n",
    "        print(f\"  Val Loss: {result['best_val_loss']:.4f}\")\n",
    "        print(f\"  Best Epoch: {result['best_epoch']}\")\n",
    "    \n",
    "    return successful_results\n",
    "\n",
    "def plot_grid_search_results(results, top_k=10):\n",
    "    \"\"\"Plot comparison of top configurations.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to plot\")\n",
    "        return\n",
    "    \n",
    "    top_results = results[:top_k]\n",
    "    \n",
    "    # Create summary plot\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Val F1 vs Train F1\n",
    "    val_f1s = [r['best_val_f1'] for r in top_results]\n",
    "    train_f1s = [r['best_train_f1'] for r in top_results]\n",
    "    config_labels = [f\"Config{i+1}\" for i in range(len(top_results))]\n",
    "    \n",
    "    ax1.scatter(val_f1s, train_f1s, alpha=0.6, s=60)\n",
    "    for i, (v, t) in enumerate(zip(val_f1s, train_f1s)):\n",
    "        ax1.annotate(f\"{i+1}\", (v, t), xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    ax1.plot([0, 1], [0, 1], 'r--', alpha=0.3)\n",
    "    ax1.set_xlabel('Validation F1'); ax1.set_ylabel('Train F1')\n",
    "    ax1.set_title('Train vs Validation F1'); ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overfitting gap\n",
    "    gaps = [r['overfitting_gap'] for r in top_results]\n",
    "    ax2.bar(range(len(gaps)), gaps, alpha=0.7)\n",
    "    ax2.set_xlabel('Configuration'); ax2.set_ylabel('Overfitting Gap (Train F1 - Val F1)')\n",
    "    ax2.set_title('Overfitting Gap by Configuration'); ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation F1 comparison\n",
    "    ax3.bar(range(len(val_f1s)), val_f1s, alpha=0.7, color='green')\n",
    "    ax3.set_xlabel('Configuration'); ax3.set_ylabel('Validation F1')\n",
    "    ax3.set_title('Validation F1 by Configuration'); ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation Loss comparison\n",
    "    val_losses = [r['best_val_loss'] for r in top_results]\n",
    "    ax4.bar(range(len(val_losses)), val_losses, alpha=0.7, color='orange')\n",
    "    ax4.set_xlabel('Configuration'); ax4.set_ylabel('Validation Loss')\n",
    "    ax4.set_title('Validation Loss by Configuration'); ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bdb59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ======================= CELL 2.5: GRID SEARCH EXECUTION =======================\n",
    "\n",
    "# Define your parameter grid - focus on regularization to combat overfitting\n",
    "PARAM_GRID = {\n",
    "    # Regularization-focused parameters\n",
    "    'attn_dropout': [0.2, 0.3, 0.4],           # Increased dropout for attention\n",
    "    'ff_dropout': [0.3, 0.4, 0.5],             # Increased dropout for FF layers  \n",
    "    'token_dropout': [0.2, 0.3, 0.4],          # Increased token dropout\n",
    "    'mlp_dropout': [0.4, 0.5, 0.6],            # Increased MLP dropout\n",
    "    \n",
    "    # Model capacity (smaller to reduce overfitting)\n",
    "    'd_token': [128, 192, 256],                # Smaller token dimensions\n",
    "    'n_layers': [4, 6],                        # Fewer layers\n",
    "    'mlp_hidden': [[256, 128], [512, 256]],    # Smaller MLP heads\n",
    "    \n",
    "    # Training regularization\n",
    "    'weight_decay': [1e-2, 2e-3, 5e-3],        # Stronger weight decay\n",
    "    'lr': [1e-4, 3e-4, 1e-3],                  # Learning rate variations\n",
    "}\n",
    "\n",
    "# Optional: Smaller grid for quick testing\n",
    "QUICK_PARAM_GRID = {\n",
    "    'attn_dropout': [0.2, 0.3],\n",
    "    'ff_dropout': [0.3, 0.4],\n",
    "    'token_dropout': [0.2, 0.3],\n",
    "    'weight_decay': [2e-3, 5e-3],\n",
    "    'd_token': [192, 256],\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "grid_results = grid_search_ft_transformer(\n",
    "    df_train=df_train, \n",
    "    df_val=df_val,\n",
    "    cont_cols=cont_cols, \n",
    "    cat_cols=cat_cols,\n",
    "    label_col=TARGET_COL, \n",
    "    num_classes=num_classes,\n",
    "    base_cfg=CFG,  # Your base configuration\n",
    "    param_grid=PARAM_GRID,  # or QUICK_PARAM_GRID for faster testing\n",
    "    max_runs=20,    # Limit number of runs for practicality\n",
    "    sort_by='val_f1'  # Sort by validation F1 score\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "if grid_results:\n",
    "    plot_grid_search_results(grid_results, top_k=10)\n",
    "    \n",
    "    # Get best configuration\n",
    "    best_config = grid_results[0]['config']\n",
    "    print(f\"\\nðŸŽ¯ BEST CONFIGURATION:\")\n",
    "    print(f\"Parameters: {best_config}\")\n",
    "    print(f\"Validation F1: {grid_results[0]['best_val_f1']:.4f}\")\n",
    "    print(f\"Overfitting gap: {grid_results[0]['overfitting_gap']:.4f}\")\n",
    "    \n",
    "    # You can now retrain with the best configuration\n",
    "    print(\"\\nTo use the best configuration, update your CFG:\")\n",
    "    print(\"CFG.update({\")\n",
    "    for k, v in best_config.items():\n",
    "        print(f\"    '{k}': {v},\")\n",
    "    print(\"})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46581597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced regularization strategies\n",
    "ENHANCED_REGULARIZATION_GRID = {\n",
    "    # Even stronger regularization\n",
    "    'attn_dropout': [0.4, 0.5],\n",
    "    'ff_dropout': [0.5, 0.6],\n",
    "    'token_dropout': [0.4, 0.5],\n",
    "    'mlp_dropout': [0.5, 0.6],\n",
    "    \n",
    "    # Architectural constraints\n",
    "    'd_token': [64, 128],\n",
    "    'n_layers': [2, 3, 4],\n",
    "    'n_heads': [4, 6],\n",
    "    'mlp_hidden': [[128, 64], [256, 128]],\n",
    "    \n",
    "    # Training strategies\n",
    "    'weight_decay': [1e-2, 2e-2],\n",
    "    'lr': [5e-5, 1e-4],\n",
    "    'batch_size': [128, 192],  # Smaller batches can regularize\n",
    "    'grad_clip_norm': [0.5, 1.0],\n",
    "}\n",
    "\n",
    "# Time-series specific strategies\n",
    "TIME_SERIES_GRID = {\n",
    "    # Add stochastic depth for deeper networks\n",
    "    'token_dropout': [0.3, 0.4, 0.5],\n",
    "    # Use layer dropout if you modify the transformer\n",
    "    'layer_dropout': [0.1, 0.2],  # You'd need to implement this\n",
    "    # Label smoothing\n",
    "    'label_smoothing': [0.1, 0.2],  # Modify loss function\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
