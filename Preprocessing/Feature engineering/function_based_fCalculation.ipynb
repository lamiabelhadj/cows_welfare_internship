{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eabd00ed"
      },
      "source": [
        "# New dataset with one hour shift and the new 32 features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d177b6d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\lamia\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Processing file: C:/Users/lamia/Desktop/datasets/dataset1_knn.csv\n",
            "Loaded data from C:/Users/lamia/Desktop/datasets/dataset1_knn.csv, shape: (105813, 15)\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'activity_window'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\lamia\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'activity_window'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 123\u001b[0m\n\u001b[0;32m    120\u001b[0m shifted_df \u001b[38;5;241m=\u001b[39m load_shifted_data(file)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shifted_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# Ensure activity_window is a list of lists\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     shifted_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_window\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m shifted_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_window\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28meval\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m shifted_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_window\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m shifted_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_window\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    124\u001b[0m     features_df \u001b[38;5;241m=\u001b[39m calculate_all_features(shifted_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_window\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    125\u001b[0m     final_df \u001b[38;5;241m=\u001b[39m combine_features(shifted_df, features_df)\n",
            "File \u001b[1;32mc:\\Users\\lamia\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32mc:\\Users\\lamia\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'activity_window'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats, fft\n",
        "\n",
        "def load_shifted_data(file_path):\n",
        "    \"\"\"Load shifted dataset and return a DataFrame.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Loaded data from {file_path}, shape: {df.shape}\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {file_path} not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def calculate_all_features(activity_windows):\n",
        "    \"\"\"Calculate 32 features for each activity window.\"\"\"\n",
        "    feature_names = [\n",
        "        'Minimum', 'Maximum', 'Mean', 'RMS', 'STD',\n",
        "        'MeanSTD6h', 'STDMean6h', 'STDSD', 'RMSSD', 'Mode',\n",
        "        'Q10', 'Q90', 'Q25', 'Q50', 'Q75',\n",
        "        'Skewness', 'Kurtosis',\n",
        "        *[f'Autocorr{i}' for i in range(1, 12)],\n",
        "        *[f'h{i}' for i in range(1, 5)]\n",
        "    ]\n",
        "    features = []\n",
        "\n",
        "    for window in activity_windows:\n",
        "        try:\n",
        "            window = np.array(window, dtype=float)\n",
        "            if len(window) == 0 or np.all(np.isnan(window)):\n",
        "                print(f\"Warning: Empty or invalid window, returning NaN features.\")\n",
        "                features.append({f: np.nan for f in feature_names})\n",
        "                continue\n",
        "\n",
        "            feature_dict = {}\n",
        "            # Time-Domain Features\n",
        "            feature_dict['Minimum'] = np.min(window)\n",
        "            feature_dict['Maximum'] = np.max(window)\n",
        "            feature_dict['Mean'] = np.mean(window)\n",
        "            feature_dict['RMS'] = np.sqrt(np.mean(np.square(window)))\n",
        "            feature_dict['STD'] = np.std(window, ddof=1) if len(window) > 1 else np.nan\n",
        "\n",
        "            # 6-hour windows\n",
        "            six_h_windows = [window[i*6:(i+1)*6] for i in range(4) if len(window[i*6:(i+1)*6]) > 0]\n",
        "            stds_6h = [np.std(w, ddof=1) if len(w) > 1 else np.nan for w in six_h_windows]\n",
        "            means_6h = [np.mean(w) if len(w) > 0 else np.nan for w in six_h_windows]\n",
        "            feature_dict['MeanSTD6h'] = np.nanmean(stds_6h) if stds_6h else np.nan\n",
        "            feature_dict['STDMean6h'] = np.std(means_6h, ddof=1) if len(means_6h) > 1 else np.nan\n",
        "\n",
        "            # Successive differences\n",
        "            diffs = np.diff(window)\n",
        "            feature_dict['STDSD'] = np.std(diffs, ddof=1) if len(diffs) > 1 else np.nan\n",
        "            feature_dict['RMSSD'] = np.sqrt(np.mean(np.square(diffs))) if len(diffs) > 0 else np.nan\n",
        "\n",
        "            # Distribution features\n",
        "            feature_dict['Mode'] = stats.mode(window, keepdims=True)[0][0] if len(window) > 0 else np.nan\n",
        "            feature_dict['Q10'] = np.percentile(window, 10) if len(window) > 0 else np.nan\n",
        "            feature_dict['Q90'] = np.percentile(window, 90) if len(window) > 0 else np.nan\n",
        "            feature_dict['Q25'] = np.percentile(window, 25) if len(window) > 0 else np.nan\n",
        "            feature_dict['Q50'] = np.percentile(window, 50) if len(window) > 0 else np.nan\n",
        "            feature_dict['Q75'] = np.percentile(window, 75) if len(window) > 0 else np.nan\n",
        "            feature_dict['Skewness'] = stats.skew(window) if len(window) > 2 else np.nan\n",
        "            feature_dict['Kurtosis'] = stats.kurtosis(window) if len(window) > 3 else np.nan\n",
        "\n",
        "            # Autocorrelation (lags 1 to 11)\n",
        "            for lag in range(1, 12):\n",
        "                if len(window) > lag and not np.all(np.isnan(window[:-lag])) and not np.all(np.isnan(window[lag:])):\n",
        "                    corr = np.corrcoef(window[:-lag], window[lag:])[0, 1]\n",
        "                else:\n",
        "                    corr = np.nan\n",
        "                feature_dict[f'Autocorr{lag}'] = corr\n",
        "\n",
        "            # Frequency-Domain Features\n",
        "            fft_result = np.abs(fft.fft(window)) if len(window) > 0 else np.array([])\n",
        "            harmonics = fft_result[1:5] if len(fft_result) > 4 else [np.nan] * 4\n",
        "            for i, h in enumerate(harmonics, 1):\n",
        "                feature_dict[f'h{i}'] = h\n",
        "\n",
        "            features.append(feature_dict)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing window: {e}\")\n",
        "            features.append({f: np.nan for f in feature_names})\n",
        "\n",
        "    return pd.DataFrame(features)\n",
        "\n",
        "def combine_features(shifted_df, features_df):\n",
        "    \"\"\"Combine features with original metadata and labels.\"\"\"\n",
        "    shifted_df_cleaned = shifted_df.drop(columns=['activity_window'])\n",
        "    features_columns = ['cow', 'start_time', 'end_time', 'duration_hours']\n",
        "    label_columns = [col for col in shifted_df_cleaned.columns if col not in features_columns]\n",
        "\n",
        "    final_df = pd.concat([\n",
        "        shifted_df_cleaned[features_columns].reset_index(drop=True),\n",
        "        features_df.reset_index(drop=True),\n",
        "        shifted_df_cleaned[label_columns].reset_index(drop=True)\n",
        "    ], axis=1)\n",
        "    return final_df\n",
        "\n",
        "def save_output(df, output_path):\n",
        "    \"\"\"Save DataFrame to CSV.\"\"\"\n",
        "    try:\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f\"CSV file saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to {output_path}: {str(e)}\")\n",
        "\n",
        "# List of input files (output from Transformation.ipynb)\n",
        "input_files = [\n",
        "    r\"C:/Users/lamia/Desktop/datasets/dataset1_knn.csv\",\n",
        "  \n",
        "]\n",
        "\n",
        "# Process each input file\n",
        "for file in input_files:\n",
        "    print(f\"\\n📊 Processing file: {file}\")\n",
        "    shifted_df = load_shifted_data(file)\n",
        "    if shifted_df is not None:\n",
        "        # Ensure activity_window is a list of lists\n",
        "        shifted_df['activity_window'] = shifted_df['activity_window'].apply(eval) if shifted_df['activity_window'].dtype == 'object' else shifted_df['activity_window']\n",
        "        features_df = calculate_all_features(shifted_df['activity_window'])\n",
        "        final_df = combine_features(shifted_df, features_df)\n",
        "        # output_path = f\"32features_1hour_shift_{file.split('_')[-1]}\"\n",
        "        output_path = r\"C:/Users/lamia/Desktop/datasets/32features_dataset1.csv\"\n",
        "        save_output(final_df, output_path)\n",
        "        print(f\"📊 Final dataset shape: {final_df.shape}\")\n",
        "        print(\"\\n🔍 First 3 rows of the final dataset:\")\n",
        "        display(final_df.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a958ec7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import stats, fft\n",
        "\n",
        "# 1. Define the feature calculation function (unchanged)\n",
        "def calculate_features(window):\n",
        "    \"\"\"Calculate all 32 features for a 24-hour window\"\"\"\n",
        "    features = {}\n",
        "    window = np.array(window)\n",
        "\n",
        "    # Time-Domain Features (1-17)\n",
        "    features['Minimum'] = np.min(window)\n",
        "    features['Maximum'] = np.max(window)\n",
        "    features['Mean'] = np.mean(window)\n",
        "    features['RMS'] = np.sqrt(np.mean(np.square(window)))\n",
        "    features['STD'] = np.std(window)\n",
        "\n",
        "    # 6-hour window statistics\n",
        "    six_h_windows = [window[i*6:(i+1)*6] for i in range(4)]\n",
        "    stds_6h = [np.std(w) for w in six_h_windows]\n",
        "    means_6h = [np.mean(w) for w in six_h_windows]\n",
        "    features['MeanSTD6h'] = np.mean(stds_6h)\n",
        "    features['STDMean6h'] = np.std(means_6h)\n",
        "\n",
        "    # Successive differences\n",
        "    diffs = np.diff(window)\n",
        "    features['RMSSD'] = np.sqrt(np.mean(np.square(diffs)))\n",
        "\n",
        "    # Quantiles and distribution shape\n",
        "    try:\n",
        "        features['Mode'] = stats.mode(window, keepdims=True)[0][0]\n",
        "    except:\n",
        "        features['Mode'] = window[0]\n",
        "\n",
        "    for p in [10, 25, 50, 75, 90]:\n",
        "        features[f'Q{p}'] = np.percentile(window, p)\n",
        "    features['Skewness'] = stats.skew(window)\n",
        "    features['Kurtosis'] = stats.kurtosis(window)\n",
        "\n",
        "    # Autocorrelations (18-28)\n",
        "    for lag in range(1, 12):\n",
        "        if len(window) > lag:\n",
        "            corr = np.corrcoef(window[:-lag], window[lag:])[0,1]\n",
        "        else:\n",
        "            corr = np.nan\n",
        "        features[f'Autocorr{lag}'] = corr\n",
        "\n",
        "    # Frequency-Domain Features (29-32)\n",
        "    fft_values = np.abs(fft.fft(window))[1:5]  # Harmonics 1-4\n",
        "    for i, h in enumerate(fft_values, 1):\n",
        "        features[f'h{i}'] = h\n",
        "\n",
        "    return features\n",
        "\n",
        "# 2. Create 1-hour shifted windows (updated to exclude unwanted columns)\n",
        "def create_shifted_windows(df, window_size=24, shift=1):\n",
        "    \"\"\"Generate overlapping 24-hour windows with 1-hour shift\"\"\"\n",
        "    shifted_data = []\n",
        "\n",
        "    # Define condition columns (excluding management_changes and OK)\n",
        "    condition_columns = ['oestrus', 'calving', 'lameness', 'mastitis', 'LPS',\n",
        "                        'acidosis', 'other_disease', 'accidents', 'disturbance',\n",
        "                        'mixing']\n",
        "\n",
        "    # Group by cow only (not date) for continuous windows\n",
        "    for cow_id, group in df.sort_values(['cow', 'date', 'hour']).groupby('cow'):\n",
        "        activity = group['ACTIVITY_LEVEL'].values\n",
        "        dates = group['date'].values\n",
        "        hours = group['hour'].values\n",
        "\n",
        "        # Generate all possible windows\n",
        "        for i in range(0, len(activity) - window_size + 1, shift):\n",
        "            window = activity[i:i + window_size]\n",
        "            conditions = group.iloc[i + window_size - 1][condition_columns].to_dict()\n",
        "\n",
        "            shifted_data.append({\n",
        "                'cow': cow_id,\n",
        "                'start_date': dates[i],\n",
        "                'start_hour': hours[i],\n",
        "                'activity_window': window.tolist(),\n",
        "                **conditions\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(shifted_data)\n",
        "\n",
        "# 3. Process your data\n",
        "print(\"Creating 1-hour shifted windows (excluding management_changes and OK)...\")\n",
        "shifted_df = create_shifted_windows(df)\n",
        "\n",
        "print(\"Calculating features for each window...\")\n",
        "feature_data = [calculate_features(w) for w in shifted_df['activity_window']]\n",
        "features_df = pd.DataFrame(feature_data)\n",
        "\n",
        "# 4. Combine into final dataset\n",
        "final_df = pd.concat([\n",
        "    shifted_df.drop(columns=['activity_window']),  # Keep only metadata and conditions\n",
        "    features_df\n",
        "], axis=1)\n",
        "\n",
        "# 5. Save the results\n",
        "output_path = \"cow_activity_features_1hour_shift_clean.csv\"\n",
        "final_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"\\nProcessing complete! Results saved to {output_path}\")\n",
        "print(f\"Final dataset shape: {final_df.shape}\")\n",
        "print(\"\\nFirst 3 rows of the final dataset:\")\n",
        "display(final_df.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a3c5acb",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
